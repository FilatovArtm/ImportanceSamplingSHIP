{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with different Bayessian Optimization approaches and Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiments, we came up with the following methods:\n",
    "1. Expected Improvement\n",
    "2. Knowledge Gradient\n",
    "3. Upper Confidence Bound\n",
    "\n",
    "with each method with provide an experiments with artificial functions where loss is estimated with importance sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "Our goal is to minimize some function $f(x)$ based only on the values of the function. Here we also assume that loss has the following form\n",
    "\n",
    "$$\n",
    "L(w) = \\sum_{i = 1}^N l(x_i, w)\n",
    "$$\n",
    "\n",
    "More over we know, that for some objects variation of the value is very low. We want to come with procedure where we can estimate $L$ properly computing loss only for $n$ objects, where $n << N$.\n",
    "\n",
    "### Importance Sampling\n",
    "\n",
    "The first attempt is importance sampling. It can be shown, that minimization of the variance guarantees the following weights in the sampling distribution. \n",
    "\n",
    "$$\n",
    "w_i = \\frac{l(x_i, w)}{\\sum l(x_j, w)}\n",
    "$$\n",
    "\n",
    "Of course, we don't have access to such values. Aassuming the low variation of the $l(x_i, w)$ with respect to $w$, we can try to estimate the optimal weight by mean over all the previous observations.\n",
    "\n",
    "### Setup\n",
    "\n",
    "In the experiments, we will work with the following function:\n",
    "\n",
    "\n",
    "We also fix the number of objects sampled. Moreover, in the practical aapplications we can automaticaly compute the optimal $n$. Look here (Lam et al. (2015), Kandasamy et al. (2016), Poloczek et al. (2017))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with artificial functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let consider that in reality we have 1000 objects. Each object is characterized by random vector in $R^2$. Then the loss function for each object will be computed as follows: \n",
    "\n",
    "$$\n",
    "l(x_i, w) = \n",
    "\\begin{cases}\n",
    "|x_i^T w| \\text{ if } Pois(|x_i^T w|) > 1 \\\\\n",
    "0 \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "We can see that in the following setup some objects will be irrelevant due to the low entries of $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
